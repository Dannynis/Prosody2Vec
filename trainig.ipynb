{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, vec_a_size, vec_b_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn_a = nn.Linear(vec_a_size, hidden_dim)  # Project vec_a to hidden_dim\n",
    "        self.attn_b = nn.Linear(vec_b_size, hidden_dim)  # Project vec_b to hidden_dim\n",
    "        self.attn_score = nn.Linear(hidden_dim, 1)  # Compute attention scores\n",
    "    \n",
    "    def forward(self, matrix, vec_a, vec_b):\n",
    "        # matrix: (batch_size, time, hidden_dim)\n",
    "        # vec_a: (batch_size, vec_a_size)\n",
    "        # vec_b: (batch_size, vec_b_size)\n",
    "        \n",
    "        batch_size, time, hidden_dim = matrix.shape\n",
    "        \n",
    "        # Project vectors into hidden space\n",
    "        a_proj = self.attn_a(vec_a).unsqueeze(1).expand(-1, time, -1)  # (batch_size, time, hidden_dim)\n",
    "        b_proj = self.attn_b(vec_b).unsqueeze(1).expand(-1, time, -1)  # (batch_size, time, hidden_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_input = torch.tanh(matrix + a_proj + b_proj)  # Combine information\n",
    "        attn_scores = self.attn_score(attn_input).squeeze(-1)  # (batch_size, time)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1).unsqueeze(-1)  # (batch_size, time, 1)\n",
    "        \n",
    "        # Apply attention to the matrix\n",
    "        updated_matrix = matrix * attn_weights  # Element-wise weighting\n",
    "        \n",
    "        return updated_matrix  # (batch_size, time, hidden_dim)\n",
    "\n",
    "# Example Usage\n",
    "batch_size, time, hidden_dim, vec_a_size, vec_b_size = 32, 10, 64, 16, 16\n",
    "matrix = torch.randn(batch_size, time, hidden_dim)\n",
    "vec_a = torch.randn(batch_size, vec_a_size)\n",
    "vec_b = torch.randn(batch_size, vec_b_size)\n",
    "\n",
    "attn = Attention(hidden_dim, vec_a_size, vec_b_size)\n",
    "output_matrix = attn(matrix, vec_a, vec_b)\n",
    "print(output_matrix.shape)  # Should be (batch_size, time, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = './Emotion Speech Dataset/'\n",
    "data_dir = '/home/dcor/niskhizov/Prosody2Vec/IEMOCAP_full_release/'\n",
    "# scan recursively for all .wav files in the data_dir\n",
    "wav_files = glob.glob(data_dir + '/**/*.wav', recursive=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
