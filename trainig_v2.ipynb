{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:59:52.683998Z",
     "start_time": "2025-02-25T19:59:52.673883Z"
    }
   },
   "outputs": [],
   "source": [
    "cd Prosody2Vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
    "tacotron2 = tacotron2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacotron2.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacotron2.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1408 - 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256 + 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacotron2.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:59:54.193693Z",
     "start_time": "2025-02-25T19:59:54.190010Z"
    }
   },
   "outputs": [],
   "source": [
    "# import clearml\n",
    "# clearml.browser_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:59:55.036200Z",
     "start_time": "2025-02-25T19:59:55.032657Z"
    }
   },
   "outputs": [],
   "source": [
    "# from clearml import Task\n",
    "# task = Task.init(project_name=\"my project\", task_name=\"my task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:59:55.406248Z",
     "start_time": "2025-02-25T19:59:55.401648Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "import glob\n",
    "from IPython.display import clear_output, display, Audio\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:18.934495Z",
     "start_time": "2025-02-25T20:00:18.929868Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:20.189933Z",
     "start_time": "2025-02-25T20:00:20.186962Z"
    }
   },
   "outputs": [],
   "source": [
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:20.763248Z",
     "start_time": "2025-02-25T20:00:20.728036Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, vec_a_size, vec_b_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn_a = nn.Linear(vec_a_size, hidden_dim)  # Project vec_a to hidden_dim\n",
    "        self.attn_b = nn.Linear(vec_b_size, hidden_dim)  # Project vec_b to hidden_dim\n",
    "        self.attn_score = nn.Linear(hidden_dim, 1)  # Compute attention scores\n",
    "    \n",
    "    def forward(self, matrix, vec_a, vec_b):\n",
    "        # matrix: (batch_size, time, hidden_dim)\n",
    "        # vec_a: (batch_size, vec_a_size)\n",
    "        # vec_b: (batch_size, vec_b_size)\n",
    "        \n",
    "        batch_size, time, hidden_dim = matrix.shape\n",
    "        \n",
    "        # Project vectors into hidden space\n",
    "        a_proj = self.attn_a(vec_a).unsqueeze(1).expand(-1, time, -1)  # (batch_size, time, hidden_dim)\n",
    "        b_proj = self.attn_b(vec_b).unsqueeze(1).expand(-1, time, -1)  # (batch_size, time, hidden_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_input = torch.tanh(matrix + a_proj + b_proj)  # Combine information\n",
    "        attn_scores = self.attn_score(attn_input).squeeze(-1)  # (batch_size, time)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1).unsqueeze(-1)  # (batch_size, time, 1)\n",
    "        \n",
    "        # Apply attention to the matrix\n",
    "        updated_matrix = matrix * attn_weights  # Element-wise weighting\n",
    "        \n",
    "        return updated_matrix  # (batch_size, time, hidden_dim)\n",
    "\n",
    "# Example Usage\n",
    "batch_size, time, hidden_dim, vec_a_size, vec_b_size = 32, 10, 64, 16, 16\n",
    "matrix = torch.randn(batch_size, time, hidden_dim)\n",
    "vec_a = torch.randn(batch_size, vec_a_size)\n",
    "vec_b = torch.randn(batch_size, vec_b_size)\n",
    "\n",
    "attn = Attention(hidden_dim, vec_a_size, vec_b_size)\n",
    "output_matrix = attn(matrix, vec_a, vec_b)\n",
    "print(output_matrix.shape)  # Should be (batch_size, time, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:22.975422Z",
     "start_time": "2025-02-25T20:00:22.666301Z"
    }
   },
   "outputs": [],
   "source": [
    "# acoustic = torch.hub.load(\"bshall/acoustic-model:main\", \"hubert_soft\", trust_repo=True).cuda()\n",
    "acoustic = model.AcousticModel().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:24.834269Z",
     "start_time": "2025-02-25T20:00:23.684772Z"
    }
   },
   "outputs": [],
   "source": [
    "hifigan = torch.hub.load(\"bshall/hifigan:main\", \"hifigan_hubert_soft\", trust_repo=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:26.202356Z",
     "start_time": "2025-02-25T20:00:25.186561Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_dir = './Emotion Speech Dataset/'\n",
    "data_dir = '/home/dcor/niskhizov/Prosody2Vec/IEMOCAP_full_release/'\n",
    "# scan recursively for all .wav files in the data_dir\n",
    "wav_files = glob.glob(data_dir + '/**/*.wav', recursive=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:28.903528Z",
     "start_time": "2025-02-25T20:00:28.899720Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_dir = 'iemocap_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:29.962486Z",
     "start_time": "2025-02-25T20:00:29.661812Z"
    }
   },
   "outputs": [],
   "source": [
    "# create pytorch dataset that loads pairs of wav a and embeddings from iemocap_embeddings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torchaudio\n",
    "\n",
    "class IemocapDataset(Dataset):\n",
    "    def __init__(self, audio_files):\n",
    "        self.audio_files = []\n",
    "        self.embeddings_file = []\n",
    "\n",
    "        for audio_file in audio_files:\n",
    "            out_file = f\"iemocap_embeddings/{audio_file.split('/')[-1].replace('.wav', '.pkl')}\"\n",
    "            if os.path.exists(out_file):                \n",
    "                self.embeddings_file.append(out_file)\n",
    "                self.audio_files.append(audio_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings_file)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        wav_path = self.audio_files[idx]\n",
    "\n",
    "        out_file = self.embeddings_file[idx]\n",
    "\n",
    "        with open(out_file, 'rb') as f:\n",
    "            embd = pickle.load(f)\n",
    "        \n",
    "        wav,sr = torchaudio.load(wav_path)\n",
    "\n",
    "        # take the first 3 seconds of the audio\n",
    "\n",
    "        wav = wav[:, :3*sr]\n",
    "\n",
    "        \n",
    "        \n",
    "        return wav, embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:30.129325Z",
     "start_time": "2025-02-25T20:00:30.107198Z"
    }
   },
   "outputs": [],
   "source": [
    "acoustic.train()\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, acoustic):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attn = Attention(hidden_dim, 1024, 192)\n",
    "        self.decoder_rnn = copy.deepcopy(acoustic)\n",
    "\n",
    "    def forward(self, units, emo_vecs, spk_vecs, logmels):\n",
    "        # units: (batch_size, time, hidden_dim)\n",
    "        # emo_vecs: (batch_size, emo_vec_size)\n",
    "        # spk_vecs: (batch_size, spk_vec_size)\n",
    "        # logmels: (batch_size, time, n_mels)\n",
    "        \n",
    "        batch_size, time, _ = units.shape\n",
    "        \n",
    "        # Apply attention\n",
    "        units_attn = self.attn(units, emo_vecs, spk_vecs)  # (batch_size, time, hidden_dim)\n",
    "        return self.decoder_rnn.generate(units_attn)\n",
    "        \n",
    "decoder = Decoder(256, acoustic).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:32.196285Z",
     "start_time": "2025-02-25T20:00:32.098361Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the latest decoder\n",
    "decoders = glob.glob('decoder_*.pth')\n",
    "decoders.sort()\n",
    "decoder.load_state_dict(torch.load(decoders[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:36.574407Z",
     "start_time": "2025-02-25T20:00:32.798452Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = IemocapDataset(wav_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = torch.utils.data.random_split(ds, [int(0.8*len(ds)), len(ds) - int(0.8*len(ds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:36.608040Z",
     "start_time": "2025-02-25T20:00:36.604902Z"
    }
   },
   "outputs": [],
   "source": [
    "# acoustic(units.cuda().unsqueeze(0), logmel.unsqueeze(0).transpose(1,2).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:36.653165Z",
     "start_time": "2025-02-25T20:00:36.647859Z"
    }
   },
   "outputs": [],
   "source": [
    "# acoustic.decoder(enc,logmel.unsqueeze(0).cuda().transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:36.737708Z",
     "start_time": "2025-02-25T20:00:36.716831Z"
    }
   },
   "outputs": [],
   "source": [
    "ds[0][1]['logmel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:40.212025Z",
     "start_time": "2025-02-25T20:00:40.204077Z"
    }
   },
   "outputs": [],
   "source": [
    "# create collate function that will pad the sequences to the same length\n",
    "def collate_fn(batch):\n",
    "    wavs = [item[0][0] for item in batch]\n",
    "    \n",
    "    units, emo_vecs, spk_vecs, logmels = [], [], [], []\n",
    "    for item in batch:\n",
    "        u = item[1]['units'][:150,:]\n",
    "        mel = item[1]['logmel'][:,:300].T\n",
    "        units.append(u)\n",
    "        emo_vecs.append(torch.tensor((item[1]['emo_vec'])))\n",
    "        spk_vecs.append(item[1]['spk_vec'])\n",
    "\n",
    "        mel  = mel[:u.size(0)*2,:]\n",
    "        # print(mel.shape)\n",
    "        mel = torch.nn.functional.pad(mel, (0,0,1,0))\n",
    "        # print(mel.shape)\n",
    "\n",
    "        logmels.append(mel)\n",
    "\n",
    "    \n",
    "    mels_lengths = torch.tensor([x.size(0) - 1 for x in logmels])\n",
    "    units_lengths = torch.tensor([x.size(0) for x in units])\n",
    "\n",
    "    units_padded = nn.utils.rnn.pad_sequence(units, batch_first=True)\n",
    "    logmels_padded = nn.utils.rnn.pad_sequence(logmels, batch_first=True)\n",
    "    \n",
    "    _,T,_ = units_padded.shape\n",
    "    # pad the sequences\n",
    "\n",
    "    wavs = nn.utils.rnn.pad_sequence(wavs, batch_first=True)\n",
    "\n",
    "    \n",
    "    return wavs, units_padded, torch.stack(emo_vecs), torch.stack(spk_vecs), logmels_padded, mels_lengths, units_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[200][1]['logmel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][1]['units'].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=10)\n",
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "optimizer = Adam(decoder.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.attn.attn_a.weight.data.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T17:58:03.480959Z",
     "start_time": "2025-02-25T17:58:03.453318Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(1000):  \n",
    "    decoder.train()\n",
    "    for idx,batch in tqdm(enumerate(dl),total=len(dl)):\n",
    "        wavs, units_padded, emo_vecs, spk_vecs, logmels_padded, mels_lengths, units_lengths  = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = decoder(units_padded.cuda(), emo_vecs.cuda(), spk_vecs.cuda(), logmels_padded[:, :-1, :].cuda())\n",
    "        # out = acoustic(units_padded.cuda(), logmels_padded[:, :-1, :].cuda())\n",
    "\n",
    "        # target = hifigan(out[:1,:,:].transpose(1, 2))\n",
    "        loss = l1_loss(out, logmels_padded[:, 1:, :].cuda(), reduction=\"none\")\n",
    "        loss = torch.sum(loss, dim=(1, 2)) / (out.size(-1) * mels_lengths.cuda())\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print('Epoch:', epoch, 'Batch:', idx)\n",
    "            print('Loss:', loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(decoder.state_dict(), f\"decoder_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T17:58:21.222134Z",
     "start_time": "2025-02-25T17:58:21.196674Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(out[0].detach().cpu().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the latest decoder\n",
    "# decoders = glob.glob('decoder_*.pth')\n",
    "# decoders.sort()\n",
    "# decoder.load_state_dict(torch.load(decoders[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs, units_padded, emo_vecs, spk_vecs, logmels_padded, mels_lengths, units_lengths  = batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    units_attn = decoder.attn(units_padded[1].unsqueeze(0).cuda(), emo_vecs[10].unsqueeze(0).cuda(), spk_vecs[10].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = decoder.decoder_rnn.generate(units_attn.cuda())\n",
    "    # out = acoustic.generate(units_padded.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rec= hifigan(out[0].T.unsqueeze(0))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(out[0].detach().cpu().numpy().T,origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(rec.squeeze().cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rec.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('./rec.wav', r.unsqueeze(0).float(),sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
